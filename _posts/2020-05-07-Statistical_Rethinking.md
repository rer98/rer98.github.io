# Statistical Rethinking 1
> A summary of Bayesian statistics for scientists by Professor Richard McElreath.

- toc: true 
- badges: true
- comments: true
- categories: [statistics, bayesian, rethinking]
<!-- - image: images/chart-preview.png -->

## Lecture 1

The long-running 'Frequentist' versus 'Bayesian' rivalry in the statistical world is over.  We are all Bayesians now.  Statistical departments across the world's universities almost exclusively approach research problems using the Bayesian paradigm now.  Other departments may be slow to follow, but are likely to catch up at some point.  The Bayesian paradigm generalizes better than the Frequentists' methods, which were originally developed by Ronald A. Fisher to deal with the specific case of agricultural problems in England in the early 20th century, where it was possible to do randomized controlled field experiments and the strength of treatment effects were large.  As the world is complex and many research fields do not have the luxury of being able to do controlled experiments (e.g. for ethical reasons or practical reasons - we cannot crash an economy to test an economic point), it is necessary to use the methods of Bayesian statistics to perform inference.  These methods allow us to incorporate our prior knowledge in a simple, practical way, while updating our knowledge as new data is revealed.  Indeed, it has been shown by Leonard Jimmie Savage in the mid 20th century that Bayesian data assimilation is the optimal way of incorporating such new data (in a 'small world' setting -see below).

Bayesian statistics, resurrected by a couple of (Cambridge?) physicists in the mid 20th century, reflects the original Laplacian way of doing statistical analysis, using the language of probability to express uncertainty in a natural, intuitive way.  At the most fundamental level, Bayesian statistics is just counting up the number of possible ways that each set of assumptions is consistent with our prior knowledge and the data (see McElreath's marbles out of a bag example).  Different sets of assumptions (each of which might be represented by a set of numerical 'parameters') have differing numbers of ways to explain the prior knowledge and the data; those with more ways are deemed more plausible.  These counts of ways can be 'normalized' (i.e. sum up the ways of all the different cases and divide each case by the total number of ways), producing a probability distribution known as the posterior distribution.  This posterior distribution thus reflects our belief about the plausibility of our assumptions (represented by the probability (density) of a specific case), and our uncertainty (represented by the shape of the probability density - are there lots of different cases that have similar plausibilities?).

A practical reason why Fisher's Frequentist view of statistics dominated the field up until recently has been the lack of computing power in order to do this 'counting'; it was necessary to use simplified mathematical analysis or comparisons to idealized cases merely to provide some basic statistical inference.  However, since the 1980s, the power of desktop computers has enabled Bayesian statistics to come into it's own.

The philosopher Karl Popper argued that scientific progress is made by falsifying hypotheses.  This is often misunderstood as requiring the researcher to compare a constructive hypothesis of interest (such as a treatment) to a 'null' (dumb) hypothesis.  However, it is the interesting constructive hypothesis that the researcher should be trying to reject, not the null hypothesis!  There is often no unique 'null' hypothesis, so which one of several null hypotheses should you pick?  Often there are competing constructive hypotheses, all sounding reasonable and they should instead be tested against each other for their plausibility.  This naturally fits in with the Bayesian framework of statistical inference.  In general, science is messy: different hypotheses can be compatible with multiple, overlapping data-generating processes / mechanisms, which can themselves produce data that are compatible with several statistical distributions.  We need distinguishability and uniqueness.  That is, we need to find data that helps to distinguish between different possibilities of the underlying processes that generated this data, and furthermore to distinguish between the potentially multiple hypotheses that imply such processes.  This may be very hard to do.  Professor McElreath highlights a (genetic?) selection debate, where more than one hypothesis is compatible with the same process model, and multiple process models can give rise to the same statistical observations.  From my own experience, there are many papers in statistical physics that highlight power laws seemingly everywhere in nature, however often the data presented could also be compatible with other 'fat-tailed' distributions.

Professor McElreath counters the claim that "all models are false but some are useful" by arguing that models can't be judged as true or false, because they are not reality (the 'large' world) but only simplified representations of it (the 'small' world) used to make inference.  He likens statistical tools to the Judaic mythical 'Golem' creatures, which can only do what we ask of them; if we are not careful about what we ask of them, we are likely to make erroneous inferences and draw dangerous conclusions.  The statistical tools are applied to the small world of the model, so no matter how powerful our inferences, if the model is not a good representation of reality then we should be skeptical of our inferences.