# Statistical Rethinking
> A summary of the bayesian statistics lecture course for scientistis by Professor Richard McElreath.

- toc: true 
- badges: true
- comments: true
- categories: [statistics, bayesian, rethinking]
<!-- - image: images/chart-preview.png -->

The frequentists versus baysians rivalry in the statistical world is over.  Statistical departments across the world generally approach research problems using the bayesian paradigm now.  Other departments may be slow to follow, but are likely to catch up soon.  The bayesian paradigm generalises better than the frequentists' methods, which were originally developed by (Ronald?) Fisher (at Cambridge?) to deal with the specific case of agricultural problems in England in the early 20th century, and where it was possible to do randomized controlled laboratory or field experiments to assess the strength of treament effects.  As the world is complex and in most cases of interesting research questions, we do not have the luxury of being able to do controlled experiments (for example, for ethical reasons or practical reasons - e.g. we cannot crash an economy to prove an economic point), it is necessary to use the methods of bayesian statistics to formulate our problem.  These methods allow us to incorporate our prior knowledge in a simple practical way, while updating out knowledge due to the revelation of new data.  Indeed, it has been shown by Savage in the mid 20th century that bayesian data assimilation is the optimal way of incorporating such new data.  

Bayesian statistics, resurrected by a couple of (again at Cambridge?) physicists in the mid 20th century, reflects the original Laplacian way of doing statistical analysis, using the language of probability to express uncertainty in a natural intuitive way.  At the most fundamental level, bayesian statistics is just counting up ('measuring') the number of possible ways that each set of assumptions is compatible with our prior knowledge and the data.  Different sets of assumptions (each of which are often represented by a set of numerical 'parameters') have different numbers of ways they can explain the prior knowledge and the data; those with more ways are deemed more plausible.  These counts of ways can be 'normalised' (i.e. sum up all the different ways and divide each case by the total number of ways), producing a probability distribution known as the posterior distribution.  This posterior distribution thus reflects our belief about the plausibility of our assumptions (what is the value of the probability denstity of a specific case or set of parameters?), and our uncertainty (are there lots of different cases that have similar plausibilities?).

A practical reason why Fisher's frequentist view of statistics dominated the field up until recently has been the lack of computing power in order to do this 'counting'; it was necessary to use mathematical analysis and simplifications / comparisons to idealised cases merely to provide some statistical inference.  However, since the 1980s, the power of desktop computers have enabled an explosion of bayesian statistical research, and should in general be the go-to method of doing statistical inference now.

The philosopher Karl Popper  argues that scientific progress is made by falsifying hypotheses.  This is often misunderstood as requiring the researcher to compare a hypothesis of interest such as a treatment to the 'null' (dumb) hypothesis.  However, why pick a stupid hypothesis that is obviously false in the first place to compare a new hypothesis against?  It is the interesting hypothesis that the researcher should try to reject!  Usually, there is no unique 'null' hypothesis, so which one should you pick?  Often there are competing hypotheses, all sounding reasonable and they should instead be tested against each other for their plausibility.  This naturally fits in with the bayesian framework of statistical inference.  In general, science is messy: multiple hypotheses can be compatible with multiple processes / mechanisms, which can themselves be compatible with multiple sets of evidence.  As researchers, we need to look for data that help to distinguish the underlying processes that generated this data, and furthermore the hypotheses that imply such processes.  We should try to find data that are compatible with the fewest underlying processes, and in the best case just a single process (uniqueness).  This may be very hard to do, as often we have to accept the data that we are given, and this data may be compatible with more than one underlying data generating process.  Professor McElreath highlights a (genetic?) selection debate, but from personal experience, there are many papers in statistical physics that highlight power laws in nature, however often the data presented could also be compatible with other 'fat-tailed' distributions.